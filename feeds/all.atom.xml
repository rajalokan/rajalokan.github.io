<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>@rajalokan</title><link href="http://rajalokan.github.io/" rel="alternate"></link><link href="http://rajalokan.github.io/feeds/all.atom.xml" rel="self"></link><id>http://rajalokan.github.io/</id><updated>2016-09-06T00:00:00+05:30</updated><subtitle>Alok Kumar</subtitle><entry><title>Theory Of Everythin... well, In Neutron!</title><link href="http://rajalokan.github.io/theory-of-everything-in-neutron.html" rel="alternate"></link><published>2016-09-06T00:00:00+05:30</published><updated>2016-09-06T00:00:00+05:30</updated><author><name>Alok Kumar</name></author><id>tag:rajalokan.github.io,2016-09-06:/theory-of-everything-in-neutron.html</id><summary type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/theory-of-everything-in-neutron/" style="color: #004085; text-decoration: underline;"&gt;theory of everything in neutron&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;In a previous blog post about getting started with neutron, we glanced through workings of OpenStack neutron. This post will allow readers to understand the details of Neutron.&lt;/p&gt;
&lt;p&gt;As part of this we will cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features.&lt;/li&gt;
&lt;li&gt;How …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/theory-of-everything-in-neutron/" style="color: #004085; text-decoration: underline;"&gt;theory of everything in neutron&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;In a previous blog post about getting started with neutron, we glanced through workings of OpenStack neutron. This post will allow readers to understand the details of Neutron.&lt;/p&gt;
&lt;p&gt;As part of this we will cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features.&lt;/li&gt;
&lt;li&gt;How is it different to older and now deprecated nova-network.&lt;/li&gt;
&lt;li&gt;Architecture of and internals.&lt;/li&gt;
&lt;li&gt;L2 Agents.&lt;/li&gt;
&lt;li&gt;L3 Agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Readers wishing to learn more should take a look at our &lt;a href="https://aptira.com/openstack-training/"&gt;OpenStack training&lt;/a&gt; courses. Both free and paid courses are available, suiting all levels of experience from beginner through to advanced. For more information please contact us.&lt;/p&gt;
&lt;h2&gt;Features&lt;/h2&gt;
&lt;p&gt;As the official wiki states:&lt;/p&gt;
&lt;p&gt;Neutron is an OpenStack project to provide “networking as a service” between interface devices (e.g., vNICs) managed by other Openstack services (e.g., nova).&lt;/p&gt;
&lt;p&gt;Essentially neutron exposes a set of APIs and provides pluggable interface to a lot of third party and community developed plugins which can be used to bring up a highly robust networking infrastructure with extended capability.&lt;/p&gt;
&lt;p&gt;In simple terms some core features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Has a Unified API &amp;amp; Core.&lt;/strong&gt; Networking can be complex because of multiple device support. Keeping API’s small make it much easier to accommodate and maintain many types of devices. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overlapping IP stack.&lt;/strong&gt; As we will find, neutron can provide isolated tenant networks within a project. This is because of overlapping IP stack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Floating IPs.&lt;/strong&gt; Gives you an ability to map an address from public range and have them attached to a VIF (Virtual Interface) of a VM.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pluggable Open Arch.&lt;/strong&gt; Many ways to achieve L2 connectivity. This type of pluggable Open Architecture allows to create such architecture.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible.&lt;/strong&gt; Routing extensions, security group extensions and LB extensions give much needed features set to neutron, but still keep the core small and maintainable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security Groups.&lt;/strong&gt; Different tenants. Neutron supports egress traffic. Logical implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Neutron vs Nova Networks&lt;/h2&gt;
&lt;p&gt;Though we will cover Neutron as part of this post, it is better to understand its predecessor. Before neutron, OpenStack networking was done by Nova network. Here is a brief comparison between the two.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron_vs_nova.png" src="images/04/neutron_vs_nova.png"&gt;&lt;/p&gt;
&lt;h2&gt;Architecture &amp;amp; Internals&lt;/h2&gt;
&lt;p&gt;Neutron is plugin extensible by design. Most of the extended features are provided by agents which can be stacked linearly for scaling. This make it very easy to maintain core features, and at the same time easily add additional features.&lt;/p&gt;
&lt;p&gt;The below diagram showcases the internals of neutrol. Neutron consists of three components, a server (API layer) and a database layer. This database is connected to a neutron-server. The neutron server can pass requests to a message queue which takes requests from neutron-server and passes on to corresponding agents.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron_architecture.png" src="images/04/neutron_architecture.png"&gt;&lt;/p&gt;
&lt;p&gt;If you are using the builtin reference implementation, you will have an L2 Agent running. For the case of providing multi tenancy and other layer 3 features, we can add L3 agents. This will be responsible for virtual routing and floating IPs.&lt;/p&gt;
&lt;p&gt;You may also need a DHCP agent which takes care of the IP stack on booting VMs. You can even break different subnets for different DHCP agents.&lt;/p&gt;
&lt;p&gt;Neutron can be further extended to provide advanced L4 to L7 features such as a Load Balancer as a service or Firewall as a service using the extra corresponding agent.&lt;/p&gt;
&lt;h3&gt;Neutron Server&lt;/h3&gt;
&lt;p&gt;If we look inside the Neutron-Server, which works as the API layer, it can be further subdivided into three major components. The first two parts of the API core are pretty obvious. One service to provide API interfaces and the other service talks to message queue.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron_api_layer_internals.png" src="images/04/neutron_api_layer_internals.png"&gt;&lt;/p&gt;
&lt;p&gt;It is the third party plugins component which is very important. This is the core of neutron.&lt;/p&gt;
&lt;h4&gt;ML2 PLUGIN&lt;/h4&gt;
&lt;p&gt;This plugin could be a monolithic core plugin, or it could be an ML2 plugin. The ML2 plugin is the default option which utilises a large number of existing L2 networking solutions. This also provides support for OVS, linux bridge and Hyper V. With this we can actually run linux bridge and OVS at the same time.&lt;/p&gt;
&lt;p&gt;This ML2 framework is also intended to greatly simplify adding support for any new L2 networking technologies, requiring much less initial and ongoing effort to add a new monolithic core plugin.&lt;/p&gt;
&lt;h4&gt;TYPE DRIVER&lt;/h4&gt;
&lt;p&gt;Based on network type such as local, flat, VLAN, GRE or VXLAN, TypeDrivers provide drivers for that particular network type. TypeDrivers provide network state for this specific type and other network validation and tenant allocation.&lt;/p&gt;
&lt;h4&gt;MECHANISM DRIVER&lt;/h4&gt;
&lt;p&gt;Each networking mechanism is managed by an ML2 MechanismDriver. The MechanismDriver is responsible for taking the information established by the TypeDriver and ensuring that it is properly applied given the specific networking mechanisms that have been enabled. For example Quota, Qos, Extra Routes, metering etc.&lt;/p&gt;
&lt;h2&gt;L2 Agents&lt;/h2&gt;
&lt;p&gt;L2 agents run on the hypervisor (compute node). Its main responsibility is to configure software bridges on those compute nodes. L2 agents sit on watcher to watch notifications that a new device has been added or removed. Apart from this, it also handles security group rules.&lt;/p&gt;
&lt;p&gt;This actually supports VLAN, GRE and VXLAN for network isolation. For the implementation with OVS, it uses ovsdb to talk to OVS for networking setup.&lt;/p&gt;
&lt;p&gt;We can better understand L2 agent using the diagram below. Whenever a new instance is spawn, nova compute adds a tap interface (a vif driver interface) and then requests neutron for a new network allocation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="l2_agent.png" src="images/04/l2_agent.png"&gt;&lt;/p&gt;
&lt;p&gt;Meanwhile the L2 agent is separately watching for any new VIF interfaces, and records the newly spawned interface. Next, it fetches details about the device attached to this interface from the neutron-server.&lt;/p&gt;
&lt;p&gt;The server then allocates a network for this resource which means creating tap interface on the VM using linux bridge and putting in security group rules (using iptables).&lt;/p&gt;
&lt;p&gt;Once the L2 agent is able to establish connection, it updates neutron-server that device is up.&lt;/p&gt;
&lt;h2&gt;L3 Agents&lt;/h2&gt;
&lt;p&gt;While L2 agents let your virtual machine get attached to your network, an L3 agent gets the routers attached to each other, providing network ready resources.&lt;/p&gt;
&lt;p&gt;All level 3 forwarding and NAT across tenant networks is done through Linux Network namespaces and iptables. This provides multiple routers with overlapping IP address. Within a cloud environment where virtual machines need to be frequently created or destroyed, neutron’s overlapping IPs are very useful feature.&lt;/p&gt;
&lt;p&gt;Apart from this, neutron’s floating IP feature allows internal Virtual Machines to connect to or from outside the public network. These floating IPs aren’t hooked to an instance, rather it is connected to the neutron port.&lt;/p&gt;
&lt;p&gt;We can use python client command line APIs to show any port associated with a neutron instance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="l3_agent.png" src="images/04/l3_agent.png"&gt;&lt;/p&gt;
&lt;p&gt;Internally, Neutron receives update requests for routers from the server. Upon getting an update request, the L3 agent puts this on a processing queue of its own.&lt;/p&gt;
&lt;p&gt;Based on your requirements, we can configure neutron with a provided feature set and extend it at a later stage.&lt;/p&gt;
&lt;p&gt;If you’d like to know more, don’t forget to checkout our &lt;a href="https://aptira.com/openstack-training/"&gt;OpenStack Training&lt;/a&gt; courses or &lt;a href="https://aptira.com/contact/"&gt;contact us&lt;/a&gt; with any questions.&lt;/p&gt;</content><category term="openstack, neutron"></category></entry><entry><title>Everything you need to know to get started with Neutron</title><link href="http://rajalokan.github.io/everything-you-need-to-know-to-get-started-with-neutron.html" rel="alternate"></link><published>2016-03-16T00:00:00+05:30</published><updated>2016-03-16T00:00:00+05:30</updated><author><name>Alok Kumar</name></author><id>tag:rajalokan.github.io,2016-03-16:/everything-you-need-to-know-to-get-started-with-neutron.html</id><summary type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/everything-need-know-get-started-neutron/" style="color: #004085; text-decoration: underline;"&gt;everything you need to konw to get started with neutron&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Understanding Neutron can be challenging if you are new to a Networking background or OpenStack. In this series of posts, we will try to cover OpenStack networking component in details. Readers wishing …&lt;/p&gt;</summary><content type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/everything-need-know-get-started-neutron/" style="color: #004085; text-decoration: underline;"&gt;everything you need to konw to get started with neutron&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Understanding Neutron can be challenging if you are new to a Networking background or OpenStack. In this series of posts, we will try to cover OpenStack networking component in details. Readers wishing to learn more should take a look at our &lt;a href="https://aptira.com/openstack-training/"&gt;OpenStack training&lt;/a&gt; courses. Both free and paid courses are available, suiting all levels of experience from beginner through to advanced. For more information, please &lt;a href="https://aptira.com/contact/"&gt;contact us&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As per the &lt;a href="http://docs.openstack.org/developer/neutron/"&gt;official OpenStack documentation&lt;/a&gt;, Neutron is an OpenStack project to provide “network connectivity as a service” between interface devices (e.g., vNICs) managed by other OpenStack services (e.g., nova).  In very simple terms neutron&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;allows users to create and manage network objects, such as networks, subnets and ports, which other OpenStack services can use through an API.&lt;/li&gt;
&lt;li&gt;It enables a large number of operators to implement complex set of networking technologies to power their network infrastructure, through the use of agents, plugins and drivers.&lt;/li&gt;
&lt;li&gt;And that is all it is. I assure this will be the last bit of heavy wordings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s try to understand networking in OpenStack with an example scenario. As part of this four section article, we will create multiple networks and subnets. Then we will spawn multiple VMs across these networks and verify network connectivity for static IPs. We will then look into restricting access to VMs using security groups and finally an insight to connectivity across different networks using routers and floating IPs. Ready to go?&lt;/p&gt;
&lt;h2&gt;Networks, Subnets &amp;amp; Ports&lt;/h2&gt;
&lt;p&gt;As promised, let’s first create some networks. For this section our plan is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create a network (GREEN). Create a subnet for this network and then spawn two VMs using this network.&lt;/li&gt;
&lt;li&gt;create another network (RED) with a different subnet and spawn another two VMs using this network&lt;/li&gt;
&lt;li&gt;verify ports created as part of above process and understand why OpenStack networking ports are useful.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image01" src="images/neutron01_01.png"&gt;&lt;/p&gt;
&lt;p&gt;On completion of this set of tasks, we will have a network infrastructure as shown above, containing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;two virtual networks(GREEN &amp;amp; RED)&lt;/li&gt;
&lt;li&gt;two subnets, one per virtual network 10.10.10.0/24 and 20.10.10.0/24&lt;/li&gt;
&lt;li&gt;four VMs (two per virtual network) with IPs dynamically assigned by DHCP on corresponding network.&lt;/li&gt;
&lt;li&gt;two DHCP hosts(automatically created if ticked while network creation)&lt;/li&gt;
&lt;li&gt;and six networking ports in use overall.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s verify first that there are no existing networks. For all the below commands, we will use python CLI bindings for OpenStack (&lt;a href="http://docs.openstack.org/developer/python-neutronclient/"&gt;python-neutronclient&lt;/a&gt; or &lt;a href="http://docs.openstack.org/developer/python-novaclient/"&gt;python-novaclient&lt;/a&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron net-list
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we create our first network named GREEN.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron net-create GREEN
Created a new network:
+---------------------------+----------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; Field                     &lt;span class="p"&gt;|&lt;/span&gt; Value                                                    &lt;span class="p"&gt;|&lt;/span&gt;
+---------------------------+----------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; admin_state_up            &lt;span class="p"&gt;|&lt;/span&gt; True                                                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; id                        &lt;span class="p"&gt;|&lt;/span&gt; 6df1fa59-25a3-4f8c-8d14-ae7f7828c1a2                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; name                      &lt;span class="p"&gt;|&lt;/span&gt; GREEN                                                    &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; provider:network_type     &lt;span class="p"&gt;|&lt;/span&gt; gre                                                      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; provider:physical_network &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; provider:segmentation_id  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;101&lt;/span&gt;                                                      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; router:external           &lt;span class="p"&gt;|&lt;/span&gt; False                                                    &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; shared                    &lt;span class="p"&gt;|&lt;/span&gt; False                                                    &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; status                    &lt;span class="p"&gt;|&lt;/span&gt; ACTIVE                                                   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; subnets                   &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tenant_id                 &lt;span class="p"&gt;|&lt;/span&gt; 34609d0ea9ce48f98145ecc5bbac9f77                         &lt;span class="p"&gt;|&lt;/span&gt;
+---------------------------------------------+----------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As we can see, a new network with name GREEN is created. This network is of type GRE with a segmentation id of 101 (this will be covered more in the next blog in this series). Also you can note that this is not an external network (looking at router:external is False).&lt;/p&gt;
&lt;p&gt;Now let’s create a subnet within this network. We chose 10.10.10.0./24 range for this subnet and a self explanatory name of 10_10_10.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron subnet-create --name 10_10_10 GREEN &lt;span class="m"&gt;10&lt;/span&gt;.10.10.0/24
Created a new subnet:
+-------------------------+----------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; Field                   &lt;span class="p"&gt;|&lt;/span&gt; Value                                                    &lt;span class="p"&gt;|&lt;/span&gt;
+-------------------------+----------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; allocation_pools        &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.2&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.254&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;           &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; cidr                    &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.10.10.0/24                                            &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; dns_nameservers         &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; enable_dhcp             &lt;span class="p"&gt;|&lt;/span&gt; True                                                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; gateway_ip              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.10.10.1                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; host_routes             &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; id                      &lt;span class="p"&gt;|&lt;/span&gt; 1667e8b9-e2fe-41a6-bfd2-7de41f777d6e                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; ip_version              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;                                                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; ipv6_address_mode       &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; ipv6_ra_mode            &lt;span class="p"&gt;|&lt;/span&gt;                                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; name                    &lt;span class="p"&gt;|&lt;/span&gt; 10_10_10                                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; network_id              &lt;span class="p"&gt;|&lt;/span&gt; 6df1fa59-25a3-4f8c-8d14-ae7f7828c1a2                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tenant_id               &lt;span class="p"&gt;|&lt;/span&gt; 34609d0ea9ce48f98145ecc5bbac9f77                         &lt;span class="p"&gt;|&lt;/span&gt;
+-------------------------+----------------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see that a subnet is created with a default allocation pool of 10.10.10.2-10.10.10.254. Any new devices in this subnet will be assigned ip addresses from this range. The first and last IPs are special and they are reserved. Here 10.10.10.1 is the default gateway as shown in the output of&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ neutron subnet-create
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;command. The last IP of this range 10.10.10.255 is the broadcast address, which is reserved for broadcast usage.&lt;/p&gt;
&lt;p&gt;Fetching the network list again, we can verify that our GREEN network has a new subnet 10.10.10.0/24.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron net-list
+-------------+-------+----------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id          &lt;span class="p"&gt;|&lt;/span&gt; name  &lt;span class="p"&gt;|&lt;/span&gt; subnets                                            &lt;span class="p"&gt;|&lt;/span&gt;
+-------------+-------+----------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; 6df --- 1a2 &lt;span class="p"&gt;|&lt;/span&gt; GREEN &lt;span class="p"&gt;|&lt;/span&gt; 1667e8b9-e2fe-41a6-bfd2-7de41f777d6e &lt;span class="m"&gt;10&lt;/span&gt;.10.10.0/24 &lt;span class="p"&gt;|&lt;/span&gt;
+-------------+-------+----------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that our basic network components are ready, let’s boot a couple of tiny VMs with this network,using the cirros image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova boot --flavor m1.tiny --image cirros --min-count &lt;span class="m"&gt;2&lt;/span&gt; --nic net-id&lt;span class="o"&gt;=&lt;/span&gt;6df1fa59-25a3-4f8c-8d14-ae7f7828c1a2 greenboxes
+--------------------------------------+-------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; Property                             &lt;span class="p"&gt;|&lt;/span&gt; Value                                           &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+-------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; OS-DCF:diskConfig                    &lt;span class="p"&gt;|&lt;/span&gt; MANUAL                                          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-AZ:availability_zone          &lt;span class="p"&gt;|&lt;/span&gt; nova                                            &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-SRV-ATTR:host                 &lt;span class="p"&gt;|&lt;/span&gt; -                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-SRV-ATTR:hypervisor_hostname  &lt;span class="p"&gt;|&lt;/span&gt; -                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-SRV-ATTR:instance_name        &lt;span class="p"&gt;|&lt;/span&gt; instance-00000016                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-STS:power_state               &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-STS:task_state                &lt;span class="p"&gt;|&lt;/span&gt; scheduling                                      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-EXT-STS:vm_state                  &lt;span class="p"&gt;|&lt;/span&gt; building                                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-SRV-USG:launched_at               &lt;span class="p"&gt;|&lt;/span&gt; -                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; OS-SRV-USG:terminated_at             &lt;span class="p"&gt;|&lt;/span&gt; -                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; accessIPv4                           &lt;span class="p"&gt;|&lt;/span&gt;                                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; accessIPv6                           &lt;span class="p"&gt;|&lt;/span&gt;                                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; adminPass                            &lt;span class="p"&gt;|&lt;/span&gt; BurRPPiCm6TK                                    &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; config_drive                         &lt;span class="p"&gt;|&lt;/span&gt;                                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; created                              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2016&lt;/span&gt;-03-09T02:04:56Z                            &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; flavor                               &lt;span class="p"&gt;|&lt;/span&gt; m1.tiny &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;                                     &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; hostId                               &lt;span class="p"&gt;|&lt;/span&gt;                                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; id                                   &lt;span class="p"&gt;|&lt;/span&gt; b2647e00-81e3-4290-b9c3-ce5e584c7265            &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; image                                &lt;span class="p"&gt;|&lt;/span&gt; cirros &lt;span class="o"&gt;(&lt;/span&gt;5c912cd4-729e-4f29-8761-eb04e630741d&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; key_name                             &lt;span class="p"&gt;|&lt;/span&gt; default                                         &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; metadata                             &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{}&lt;/span&gt;                                              &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; name                                 &lt;span class="p"&gt;|&lt;/span&gt; greenboxes-b2647e00-81e3-4290-b9c3-ce5e584c7265 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; os-extended-volumes:volumes_attached &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[]&lt;/span&gt;                                              &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; progress                             &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;                                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; security_groups                      &lt;span class="p"&gt;|&lt;/span&gt; default                                         &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; status                               &lt;span class="p"&gt;|&lt;/span&gt; BUILD                                           &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tenant_id                            &lt;span class="p"&gt;|&lt;/span&gt; 34609d0ea9ce48f98145ecc5bbac9f77                &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; updated                              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2016&lt;/span&gt;-03-09T02:04:56Z                            &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; user_id                              &lt;span class="p"&gt;|&lt;/span&gt; e9b23b6cb7ef4e7c99d8934e34163726                &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+-------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This results in a message stating that two m1.tiny VMs with images cirros are building.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova list --fields Name,Networks
+--------------------------------------+----------------------------------+------------------+
&lt;span class="p"&gt;|&lt;/span&gt; ID                                   &lt;span class="p"&gt;|&lt;/span&gt; Name                             &lt;span class="p"&gt;|&lt;/span&gt; Networks         &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+----------------------------------+------------------+
&lt;span class="p"&gt;|&lt;/span&gt; b2647e00-81e3-4290-b9c3-ce5e584c7265 &lt;span class="p"&gt;|&lt;/span&gt; greenboxes-b2647e00-ce5e584c7265 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;GREEN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;.10.10.5 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; f0d6e68c-5e18-432c-b0a7-4ef6da054053 &lt;span class="p"&gt;|&lt;/span&gt; greenboxes-f0d6e68c-4ef6da054053 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;GREEN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;.10.10.6 &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+----------------------------------+------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A quick and short &lt;code&gt;nova list&lt;/code&gt; above shows that the VMs have booted with two IPs in the 10_10_10 subnet. As part of the boot process, a port on the network virtual switch is attached to the virtual interface of the VM. Taking a look at the ports for this network should give us an understanding of the devices that have been added to this network.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron port-list -c id -c fixed_ips
+-----+-----------------------------------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id  &lt;span class="p"&gt;|&lt;/span&gt; fixed_ips                                                                         &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+--------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.5&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;
+-----+-----------------------------------------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hitting &lt;code&gt;neutron port-list&lt;/code&gt; above shows that there are three ports created. Two attached to VM interfaces and one attached to a virtual DHCP server. This DHCP server is created because we didn’t disable DHCP while creating the network. This DHCP service is responsible for allocating dynamic IP addresses to virtual machines.&lt;/p&gt;
&lt;p&gt;Another, easier way to verify port creation is from the horizon dashboard. On clicking on a a particular network, we are taken to the network details page where we can see the ports being used for the selected network. For example, upon opening the network details page of network GREEN, we see&lt;/p&gt;
&lt;p&gt;We can see two Compute:None ports which tell us that those ports are connected to VMs. And one (10.10.10.4) port which is connected to the DHCP service.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Network in OpenStack is kind of a VLAN but with more flexibility.&lt;/p&gt;
&lt;p&gt;A Subnet is a block of IP addresses and associated configuration state. Subnets are used to allocate IP addresses when new ports are created on a network. &amp;amp;&lt;/p&gt;
&lt;p&gt;A Neutron Port is a connection point for attaching a single device, such as the NIC of a virtual server, to a virtual network. The port also describes the associated network configuration, such as the MAC and IP addresses to be used on that port.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that our first set of VMs and network components have been created, similarly we will create another new network (named RED), create another new subnet for this network and boot two new VMs using these networks and subnets. Assuming we have repeated the above steps, we can verify four instances in the  horizon dashboard, as below. These four instances are running in a set of two nodes, two connected to the GREEN network(10.10.10.X) and the other two connected to the RED (20.10.10.X) network.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image03" src="image03"&gt;&lt;/p&gt;
&lt;p&gt;Four instances running, two in GREEN network(10.10.10.X) &amp;amp; other two in RED (20.10.10.X) network.&lt;/p&gt;
&lt;p&gt;There exist two networks&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron net-list
+--------------------------------------+-------+-----------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id                                   &lt;span class="p"&gt;|&lt;/span&gt; name  &lt;span class="p"&gt;|&lt;/span&gt; subnets                                 &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+-------+-----------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; 6df1fa59-25a3-4f8c-8d14-ae7f7828c1a2 &lt;span class="p"&gt;|&lt;/span&gt; GREEN &lt;span class="p"&gt;|&lt;/span&gt; 1667e8b9--7de41f777d6e &lt;span class="m"&gt;10&lt;/span&gt;.10.10.0/24    &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; 1048605e-41f6-4c10-be89-935f836a1b40 &lt;span class="p"&gt;|&lt;/span&gt; RED   &lt;span class="p"&gt;|&lt;/span&gt; db05f7e4--d9e164f86ca8 &lt;span class="m"&gt;20&lt;/span&gt;.10.10.0/24    &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+-------+-----------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A total of six ports are created. Four ports for the four VMs and two ports to connect to two DHCP services per network.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron port-list -c id -c fixed_ips
+-----+----------------------------------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id  &lt;span class="p"&gt;|&lt;/span&gt; fixed_ips                                                                        &lt;span class="p"&gt;|&lt;/span&gt;
+-----+----------------------------------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.5&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XXX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.5&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
+-----+----------------------------------------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As part of this section, we have created our network infrastructure as promised in the beginning of this section. This shows that with the help of Neutron Networks, Subnets and Ports, we can create a highly complex but flexible networking setup to match our needs.&lt;/p&gt;
&lt;h2&gt;Security Groups&lt;/h2&gt;
&lt;p&gt;In the previous section, even though the VMs are created and connected within same network, we might not be able to ping or ssh due to security groups. These are a virtual firewall for your compute instances to control inbound and outbound traffic. Security Groups in OpenStack are implemented per VM. You can create a bunch of security group rules and assign them to instances. There will be more in-depth details about security groups in future article of this series.&lt;/p&gt;
&lt;p&gt;Let’s list all security groups&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova secgroup-list
+--------------------------------------+----------+----------------+
&lt;span class="p"&gt;|&lt;/span&gt; Id                                   &lt;span class="p"&gt;|&lt;/span&gt; Name     &lt;span class="p"&gt;|&lt;/span&gt; Description    &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+----------+----------------+
&lt;span class="p"&gt;|&lt;/span&gt; 4be520ef-58ad-4fa2-a471-2721290b88d7 &lt;span class="p"&gt;|&lt;/span&gt; default  &lt;span class="p"&gt;|&lt;/span&gt; default        &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+----------+----------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And to list each rules for one such security group default&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova secgroup-list-rules default
+----------------+-----------+---------+------------+--------------+
&lt;span class="p"&gt;|&lt;/span&gt; IP Protocol    &lt;span class="p"&gt;|&lt;/span&gt; From Port &lt;span class="p"&gt;|&lt;/span&gt; To Port &lt;span class="p"&gt;|&lt;/span&gt; IP Range   &lt;span class="p"&gt;|&lt;/span&gt; Source Group &lt;span class="p"&gt;|&lt;/span&gt;
+----------------+-----------+---------+------------+--------------+
&lt;span class="p"&gt;|&lt;/span&gt;                &lt;span class="p"&gt;|&lt;/span&gt;           &lt;span class="p"&gt;|&lt;/span&gt;         &lt;span class="p"&gt;|&lt;/span&gt; default    &lt;span class="p"&gt;|&lt;/span&gt;              &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; icmp           &lt;span class="p"&gt;|&lt;/span&gt; -1        &lt;span class="p"&gt;|&lt;/span&gt; -1      &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0  &lt;span class="p"&gt;|&lt;/span&gt;              &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tcp            &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;65535&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0  &lt;span class="p"&gt;|&lt;/span&gt;              &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt;                &lt;span class="p"&gt;|&lt;/span&gt;           &lt;span class="p"&gt;|&lt;/span&gt;         &lt;span class="p"&gt;|&lt;/span&gt;            &lt;span class="p"&gt;|&lt;/span&gt; default      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; udp            &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;65535&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0  &lt;span class="p"&gt;|&lt;/span&gt;              &lt;span class="p"&gt;|&lt;/span&gt;
+----------------+-----------+---------+-------------+-------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let’s find out security group of any one of VM.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova show greenboxes-b2647e00-81e3-4290-b9c3-ce5e584c7265 &lt;span class="p"&gt;|&lt;/span&gt; grep security_groups
&lt;span class="p"&gt;|&lt;/span&gt; security_groups                      &lt;span class="p"&gt;|&lt;/span&gt; default                   &lt;span class="p"&gt;|&lt;/span&gt;
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This suggests, our VM1 (greenboxes-b2647e00-81e3-4290-b9c3-ce5e584c7265) is booted with default security group. Now let’s add a rule to default to allow ssh connections(port 22).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova secgroup-add-rule default tcp &lt;span class="m"&gt;22&lt;/span&gt; &lt;span class="m"&gt;22&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0
+----------------+-----------+---------+------------+------------+
&lt;span class="p"&gt;|&lt;/span&gt; IP Protocol    &lt;span class="p"&gt;|&lt;/span&gt; From Port &lt;span class="p"&gt;|&lt;/span&gt; To Port &lt;span class="p"&gt;|&lt;/span&gt; IP Range &lt;span class="p"&gt;|&lt;/span&gt; Source Group &lt;span class="p"&gt;|&lt;/span&gt;
+----------------+-----------+---------+------------+------------+
&lt;span class="p"&gt;|&lt;/span&gt; tcp            &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;22&lt;/span&gt;        &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;22&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0  &lt;span class="p"&gt;|&lt;/span&gt;            &lt;span class="p"&gt;|&lt;/span&gt;
+----------------+-----------+---------+------------+------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; nova secgroup-list-rules default &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="m"&gt;22&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tcp            &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;22&lt;/span&gt;        &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;22&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0  &lt;span class="p"&gt;|&lt;/span&gt;            &lt;span class="p"&gt;|&lt;/span&gt;
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Listing security groups for default security groups, we can verify that default secgrup has a rule for port 22. This means on any virtual machines booted with default security group, port 22  will be open for incoming connection and hence we can make ssh connection. These security group rules are project specific, and project members can edit the default rules for their group and add new rules sets.&lt;/p&gt;
&lt;h2&gt;Routers&lt;/h2&gt;
&lt;p&gt;In the above diagram, instances within the network GREEN can talk to any other devices as long as they are in same network. This means that VM1 can send and receive packets from VM2, but not from VM3 or VM4. To provide connectivity between two different networks, routers come into the picture.&lt;/p&gt;
&lt;p&gt;Routers are logical networking components which&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;forward data packets between networks,&lt;/li&gt;
&lt;li&gt;provide L3 and NAT forwarding to provide external access for VMs on tenant networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s demo these two features of routers. In this section, we will introduce a new router. Attach one end of each network to a router so that they are connected. Then we will create a dummy external network and make this router a gateway. This way all our VMs can communicate with the outside world. At the end of this section, we will have a setup as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron01_4" src="images/neutron01_4.png"&gt;&lt;/p&gt;
&lt;p&gt;To connect any two networks we need a router. Let’s create an external virtual router named EXT_VR.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron router-create EXT_VR
Created a new router:
+-----------------------+--------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; Field                 &lt;span class="p"&gt;|&lt;/span&gt; Value                                &lt;span class="p"&gt;|&lt;/span&gt;
+-----------------------+--------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; admin_state_up        &lt;span class="p"&gt;|&lt;/span&gt; True                                 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; distributed           &lt;span class="p"&gt;|&lt;/span&gt; False                                &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; external_gateway_info &lt;span class="p"&gt;|&lt;/span&gt;                                      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; ha                    &lt;span class="p"&gt;|&lt;/span&gt; False                                &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; id                    &lt;span class="p"&gt;|&lt;/span&gt; c326b2b4-9076-4e12-9850-77d5f1c66c86 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; name                  &lt;span class="p"&gt;|&lt;/span&gt; EXT_VR                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; routes                &lt;span class="p"&gt;|&lt;/span&gt;                                      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; status                &lt;span class="p"&gt;|&lt;/span&gt; ACTIVE                               &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tenant_id             &lt;span class="p"&gt;|&lt;/span&gt; 34609d0ea9ce48f98145ecc5bbac9f77     &lt;span class="p"&gt;|&lt;/span&gt;
+-----------------------+--------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we need to add the virtual interfaces of each network to this recently created router EXT_VR. This way we can have a network with multiple subnets and decide which subnet to connect to router.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron subnet-list -c id -c name -c cidr
+--------------------------------------+----------+---------------+
&lt;span class="p"&gt;|&lt;/span&gt; id                                   &lt;span class="p"&gt;|&lt;/span&gt; name     &lt;span class="p"&gt;|&lt;/span&gt; cidr          &lt;span class="p"&gt;|&lt;/span&gt;
+------------ -------------------------+----------+---------------+
&lt;span class="p"&gt;|&lt;/span&gt; 1667e8b9-e2fe-41a6-bfd2-7de41f777d6e &lt;span class="p"&gt;|&lt;/span&gt; 10_10_10 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.10.10.0/24 &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; db05f7e4-2d48-4d86-8089-d9e164f86ca8 &lt;span class="p"&gt;|&lt;/span&gt; 20_10_10 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;.10.10.0/24 &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+----------+---------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron router-interface-add c326b2b4-9076-4e12-9850-77d5f1c66c86 10_10_10
Added interface 64b09b83-1500-4caf-b9ea-ea4c8ec127d7 to router c326b2b4-9076-4e12-9850-77d5f1c66c86.
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron router-interface-add c326b2b4-9076-4e12-9850-77d5f1c66c86 20_10_10
Added interface 0b1e3767-363e-49ba-8ee9-33c02db354bc to router c326b2b4-9076-4e12-9850-77d5f1c66c86.
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that both these networks are connected through router EXT_VR, we can verify connectivity.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cirros@vm1 $  ping -c &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.10.10.6
PING &lt;span class="m"&gt;10&lt;/span&gt;.10.10.6 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;.10.10.6&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;56&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;84&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; bytes of data.
&lt;span class="m"&gt;64&lt;/span&gt; bytes from &lt;span class="m"&gt;10&lt;/span&gt;.10.10.6: &lt;span class="nv"&gt;icmp_seq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt; &lt;span class="nv"&gt;time&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.102 ms
&lt;span class="m"&gt;64&lt;/span&gt; bytes from &lt;span class="m"&gt;10&lt;/span&gt;.10.10.6: &lt;span class="nv"&gt;icmp_seq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt; &lt;span class="nv"&gt;time&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.120 ms
--- &lt;span class="m"&gt;10&lt;/span&gt;.10.10.6 ping statistics ---
&lt;span class="m"&gt;2&lt;/span&gt; packets transmitted, &lt;span class="m"&gt;2&lt;/span&gt; received, &lt;span class="m"&gt;0&lt;/span&gt;% packet loss, &lt;span class="nb"&gt;time&lt;/span&gt; 999ms
rtt min/avg/max/mdev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.102/0.111/0.120/0.009 ms
$ cirros@vm1 $
$ cirros@vm1 $  ping -c &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;.10.10.5
PING &lt;span class="m"&gt;20&lt;/span&gt;.10.10.5 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;.10.10.5&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;56&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;84&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; bytes of data.
&lt;span class="m"&gt;64&lt;/span&gt; bytes from &lt;span class="m"&gt;20&lt;/span&gt;.10.10.5: &lt;span class="nv"&gt;icmp_seq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt; &lt;span class="nv"&gt;time&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.132 ms
&lt;span class="m"&gt;64&lt;/span&gt; bytes from &lt;span class="m"&gt;20&lt;/span&gt;.10.10.5: &lt;span class="nv"&gt;icmp_seq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt; &lt;span class="nv"&gt;time&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.180 ms
--- &lt;span class="m"&gt;20&lt;/span&gt;.10.10.5 ping statistics ---
&lt;span class="m"&gt;2&lt;/span&gt; packets transmitted, &lt;span class="m"&gt;2&lt;/span&gt; received, &lt;span class="m"&gt;0&lt;/span&gt;% packet loss, &lt;span class="nb"&gt;time&lt;/span&gt; 999ms
rtt min/avg/max/mdev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.132/0.156/0.180/0.024 ms
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Even though we can see connectivity between virtual machines across different networks, there is still no Internet connectivity. Packets flowing from virtual machines can still not reach to any outside network. Our current state of network infrastructure looks like&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron01_4_02" src="images/neutron01_4_02.png"&gt;&lt;/p&gt;
&lt;p&gt;Here two networks (RED &amp;amp; GREEN) are connected to the router but there is no connectivity between the router and the Internet as shown above. To do that we need to set this router as a gateway, which means connecting one interface of the router to the external network. In our case we have an existing external network, and we will add this network as a gateway for this router.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron router-gateway-set EXT_VR ext
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will create a new port which will attach the external network to the router. Let’s verify this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron port-list -c id -c fixed_ips
+----+----------------------------------------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id &lt;span class="p"&gt;|&lt;/span&gt; fixed_ips                                                                              &lt;span class="p"&gt;|&lt;/span&gt;
+----+----------------------------------------------------------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;acd4141a-149d-4a11-80bb-17eabaf0149f&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;192.168.100.104&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.5&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.1&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1667e8b9-e2fe-41a6-bfd2-7de41f777d6e&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;10.10.10.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.5&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;acd4141a-149d-4a11-80bb-17eabaf0149f&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;192.168.100.105&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; XX &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;subnet_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;db05f7e4-2d48-4d86-8089-d9e164f86ca8&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ip_address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;20.10.10.1&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
+----+----------------------------------------------------------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As we can see, there are two new ports (for 192.168.100.104 and for 192.168.100.105). These are the two new connection points, one attached router to external network and other external network to Internet. And now packets from inside the virtual machine can reach the internet.&lt;/p&gt;
&lt;h2&gt;Floating IPs&lt;/h2&gt;
&lt;p&gt;We have seen that the VMs from the previous step are able to reach outside public networks. But the opposite is not yet possible, we cannot reachthe VMs from an outside network. Any packet directed to say VM1 (having private IP 10.10.0.5) reaches the outside router but this gateway router has no way to distinguish target VM. Let’s try to understand this in more detail.&lt;/p&gt;
&lt;p&gt;&lt;img alt="neutron01_5_1.png" src="images/neutron01_5-1.png"&gt;&lt;/p&gt;
&lt;p&gt;The above diagram is similar to our previous network infrastructure. Initially we try sending a packet from VM1 to an outside machine (with IP 56.57.58.59). Since VM1 is connected to the virtual router for external connectivity, the packet is able to reach the virtual router with a source IP of 10.10.10.5 and a target IP of 56.57.58.59. Our virtual external router EXT_VR then changes the source IP to its own IP address (192.168.0.19) and sends the packets to the correct destination. This is possible by sourcenet.&lt;/p&gt;
&lt;p&gt;But for packet transfer from any outside source to any one of the VMs, packets can reach our external virtual router EXT_VR using the router’s public IP address. But based on the information provided in the packets, the virtual router has no way to identify the target VM.&lt;/p&gt;
&lt;p&gt;One way to allow connectivity for a VM from outside networks is creating public IPs for each of the VMs. But this is not maintainable in the long term. In general we want our VMs to be disconnected from the public network except occasionally. Using public IPs per VM will lead us to a situation where we have to maintain a large pool of IPs per VM.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Floating IPs are routable public IPs which can be assigned to a VM and revoked again. This is maintained on the router level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As part of this section, let’s generate some floating IPs in the public network EXT. Then we will dynamically assign them to VM1 and verify connectivity.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron floatingip-list
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron floatingip-create ext
Created a new floatingip:
+---------------------+----------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; Field               &lt;span class="p"&gt;|&lt;/span&gt; Value                                  &lt;span class="p"&gt;|&lt;/span&gt;
+---------------------+----------------------------------------+
&lt;span class="p"&gt;|&lt;/span&gt; fixed_ip_address    &lt;span class="p"&gt;|&lt;/span&gt;                                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; floating_ip_address &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;192&lt;/span&gt;.168.100.103                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; floating_network_id &lt;span class="p"&gt;|&lt;/span&gt; 4901039d-07b1-4ebf-91f1-559dd657e034   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; id                  &lt;span class="p"&gt;|&lt;/span&gt; 02ff1d61-6280-4f25-a8ff-9e5676ece01d   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; port_id             &lt;span class="p"&gt;|&lt;/span&gt;                                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; router_id           &lt;span class="p"&gt;|&lt;/span&gt;                                        &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; status              &lt;span class="p"&gt;|&lt;/span&gt; DOWN                                   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; tenant_id           &lt;span class="p"&gt;|&lt;/span&gt; 34609d0ea9ce48f98145ecc5bbac9f77       &lt;span class="p"&gt;|&lt;/span&gt;
+---------------------+----------------------------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron floatingip-associate 02ff1d61-6280-4f25-a8ff-9e5676ece01d 378956ec-3928-4e06-873c-423b7f017695
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt;
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; neutron floatingip-list -c id -c fixed_ip_address -c floating_ip_address
+---------------------- ---------------+------------------+---------------------+
&lt;span class="p"&gt;|&lt;/span&gt; id                                   &lt;span class="p"&gt;|&lt;/span&gt; fixed_ip_address &lt;span class="p"&gt;|&lt;/span&gt; floating_ip_address &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+------------------+---------------------+
&lt;span class="p"&gt;|&lt;/span&gt; 02ff1d61-6280-4f25-a8ff-9e5676ece01d &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.10.10.5       &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;192&lt;/span&gt;.168.100.103     &lt;span class="p"&gt;|&lt;/span&gt;
+--------------------------------------+------------------+---------------------+
$ root@controller &lt;span class="o"&gt;(&lt;/span&gt;admin&lt;span class="o"&gt;)&lt;/span&gt; $&amp;gt; &lt;span class="c1"&gt;#...................................................................&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The initial neutron &lt;code&gt;floatingip-list&lt;/code&gt; command gives no result. This means there is no existing floating ips. Then we generated a floating-ip on ext network. This IP will be randomly selected from the pool of network addresses for network ext. We find out the port of VM we want it to attach to. Using neutron &lt;code&gt;floatingip-associate&lt;/code&gt;, we can associate this floating IP with a virtual machine port. This establishes the connection to VM1 from an outside network.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This brings us to the end of this large getting started with Neutron guide. We now have an understanding of how to create networks and subnets in an OpenStack cloud. We also looked at blocking traffic per virtual machine through the use of security groups. We saw how to connect different networks using L3 switching and routers, and we used floating IPs to allow external connectivity to virtual machines.&lt;/p&gt;
&lt;p&gt;If you’d like to know more, don’t forget to checkout our &lt;a href="https://aptira.com/openstack-training/"&gt;OpenStack training&lt;/a&gt; courses or &lt;a href="https://aptira.com/contact/"&gt;contact us&lt;/a&gt; with any questions.&lt;/p&gt;</content><category term="openstack, neutron"></category></entry><entry><title>Getting Started with coreos in OpenStack</title><link href="http://rajalokan.github.io/getting-started-with-coreos-in-openstack.html" rel="alternate"></link><published>2015-08-13T14:39:00+05:30</published><updated>2015-08-13T14:39:00+05:30</updated><author><name>Alok Kumar</name></author><id>tag:rajalokan.github.io,2015-08-13:/getting-started-with-coreos-in-openstack.html</id><summary type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/getting-started-with-coreos-on-openstack/" style="color: #004085; text-decoration: underline;"&gt;getting started with coreos on openstack&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;CoreOS is an open-source lightweight operating system based on the Linux kernel and is designed to provide infrastructure for clustered deployments.&lt;/p&gt;
&lt;p&gt;Microservices architecture have their advantages. In case you are building/managing your stack as containerized …&lt;/p&gt;</summary><content type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/getting-started-with-coreos-on-openstack/" style="color: #004085; text-decoration: underline;"&gt;getting started with coreos on openstack&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;CoreOS is an open-source lightweight operating system based on the Linux kernel and is designed to provide infrastructure for clustered deployments.&lt;/p&gt;
&lt;p&gt;Microservices architecture have their advantages. In case you are building/managing your stack as containerized microservices, CoreOS is the perfect operating system. CoreOS provides only the minimal functionality required for deploying applications inside software containers, together with built-in mechanisms for service discovery and configuration sharing.&lt;/p&gt;
&lt;p&gt;Getting started with coreos on openstack (using HEAT)This blog post shares my experiences while learning the basics of CoreOS on an OpenStack infrastructure. On completion of the article, we will have a three node cluster comprised of one control node and two worker nodes. Provisioning of the cluster will be done using OpenStack Heat templates.&lt;/p&gt;
&lt;p&gt;On right is a simple graphics of our end cluster setup.&lt;/p&gt;
&lt;p&gt;The code snippets used in the post are available in my public github account https://github.com/rajalokan/coreos-openstack-beginner for reference.
&lt;img alt="getting_started_with_coreos" src="images/getting_started_with_coreos.png"&gt;&lt;/p&gt;
&lt;h2&gt;PREREQUISITES:&lt;/h2&gt;
&lt;p&gt;To follow this tutorial, we need to install some binaries on the local machine.&lt;/p&gt;
&lt;h3&gt;ETCDCTL&lt;/h3&gt;
&lt;p&gt;etcdctl is a command line client for etcd. CoreOS’s etcd is a distributed, consistent key-value store for shared configuration and service discovery.&lt;/p&gt;
&lt;p&gt;Our control node will have the etcd service running, so on the local machine we need the etcd client installed in order to talk to the CoreOS cluster. We can install this in one step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ curl -L  https://github.com/coreos/etcd/releases/download/v2.1.1/etcd-v2.1.1-linux-amd64.tar.gz -o /tmp/etcd-v2.1.1-linux-amd64.tar.gz
$ tar xzvf /tmp/etcd-v2.1.1-linux-amd64.tar.gz -C /tmp/
$ mv /tmp/etcd-v2.1.1-linux-adm64/etcdctl /usr/local/bin &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x /usr/local/bin/etcdctl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Make sure to add /usr/local/bin to the system PATH or call it directly using &lt;code&gt;/usr/local/bin/etcdctl&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;fleetctl&lt;/h3&gt;
&lt;p&gt;fleet ties together systemd and etcd into a simple distributed init system. Think of it as an extension of systemd that operates at the cluster level instead of at the machine level.&lt;/p&gt;
&lt;p&gt;fleet provides a command-line tool called &lt;code&gt;fleetctl&lt;/code&gt;. We will use this to communicate with our cluster. To install, run the following commands.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ curl -L https://github.com/coreos/fleet/releases/download/v0.11.2/fleet-v0.11.2-linux-amd64.tar.gz -o fleet-v0.11.2-linux-amd64
$ tar xzvf /tmp/fleet-v0.11.2-linux-amd64 -C /tmp/
$ mv /tmp/fleet-v0.11.2-linux-amd64/fleetctl /usr/local/bin &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x /usr/local/bin/fleetctl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;python-heatclient &amp;amp; python-glanceclient&lt;/h3&gt;
&lt;p&gt;We will use the OpenStack heat client to spin up a VM and the OpenStack glance client to create a CoreOS image in our OpenStack infrastructure. We can install both of these client tools using pip:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ pip install python-heatclient python-glanceclient
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we want to add the stable CoreOS image to glance&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ wget http://stable.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2
$ bunzip2 coreos_production_openstack_image.img.bz2
$ glance image-create –name CoreOS –container-format bare  –disk-format qcow2  –file coreos_production_openstack_image.img
$ glance image-list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we should verify that all the binaries are installed correctly. Each of the following commands &lt;code&gt;fleetctl –version&lt;/code&gt; , &lt;code&gt;etcdctl –version&lt;/code&gt;, &lt;code&gt;heat –version&lt;/code&gt; &amp;amp; &lt;code&gt;glance –version&lt;/code&gt; should return output with the version of the binary installed.&lt;/p&gt;
&lt;h2&gt;START A SINGLE NODE CLUSTER&lt;/h2&gt;
&lt;p&gt;Lets keep things simple and start by spinning up a single node cluster (only the control node). This will start a single CoreOS node with fleet and etcd running on it.&lt;/p&gt;
&lt;h3&gt;Start the cluster&lt;/h3&gt;
&lt;p&gt;Starting a cluster through HEAT requires three parameters.&lt;/p&gt;
&lt;h4&gt;Discovery token:&lt;/h4&gt;
&lt;p&gt;For a group of CoreOS machines to form a cluster, their etcd instances need to be connected. We are creating a discovery token for single node to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL.&lt;/p&gt;
&lt;p&gt;In our example, we are using CoreOS’s discovery service to generate token ( curl -q https://discovery.etcd.io/new?size=1). We can as well use our own mechanism to generate token.&lt;/p&gt;
&lt;h4&gt;public network uuid&lt;/h4&gt;
&lt;p&gt;Pass your unique identification of your public network of your OpenStack infrastructure. This is needed to create router inside public network.&lt;/p&gt;
&lt;h4&gt;key_name&lt;/h4&gt;
&lt;p&gt;Also provide your nova keypair’s key_name for ssh access.&lt;/p&gt;
&lt;p&gt;The exact command to start a coreos cluster is shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-create -f heat-template-control.yaml -P &lt;span class="nv"&gt;discovery_token_url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;curl -q https://discovery.etcd.io/new?size&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt; -P &lt;span class="nv"&gt;public_network_uuid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;87cb4819-33d4-4f2d-86d2-6970c11962da trycoreos

+————————————————————+—————–+——————————+————————————-+
&lt;span class="p"&gt;|&lt;/span&gt; id                                                                       &lt;span class="p"&gt;|&lt;/span&gt; stack_name &lt;span class="p"&gt;|&lt;/span&gt; stack_status                 &lt;span class="p"&gt;|&lt;/span&gt;  creation_time                      &lt;span class="p"&gt;|&lt;/span&gt;
+————————————————————+—————–+——————————+————————————-+
&lt;span class="p"&gt;|&lt;/span&gt; 897f08ad-4beb-4000-a871-aaa0231ade90     &lt;span class="p"&gt;|&lt;/span&gt; trycoreos      &lt;span class="p"&gt;|&lt;/span&gt; CREATE_IN_PROGRES   &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2015&lt;/span&gt;-07-30T22:11:35Z        &lt;span class="p"&gt;|&lt;/span&gt;
+————————————————————+—————–+——————————+————————————-+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The initial ‘CREATE_IN_PROGRESS’ stack_status shows that VM provisioning has started. The stack status can be checked with heat stack-show trycoreos. A ‘CREATE_COMPLETE’ stack_status means that our cluster is up. To get the floating ip address of the control node use the command heat output-show trycoreos control_ip.&lt;/p&gt;
&lt;h3&gt;Check the cluster status&lt;/h3&gt;
&lt;p&gt;Now that we have our node ready, we can ssh to it directly by &lt;code&gt;ssh core@&amp;lt;ip_address&amp;gt;&lt;/code&gt;. But let’s try using fleetctl instead to see the status of our cluster and to ssh into the VMs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$  &lt;span class="nv"&gt;FLEETCTL_TUNNEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”108.182.62.205″ fleetctl list-machines

+—————————–+—————————-+————————+
&lt;span class="p"&gt;|&lt;/span&gt;  MACHINE                    &lt;span class="p"&gt;|&lt;/span&gt;        IP                        &lt;span class="p"&gt;|&lt;/span&gt;       METADATA       &lt;span class="p"&gt;|&lt;/span&gt;
+—————————–+—————————-+————————+
&lt;span class="p"&gt;|&lt;/span&gt;  0315e138…                &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;192&lt;/span&gt;.168.222.2           &lt;span class="p"&gt;|&lt;/span&gt;       &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;control   &lt;span class="p"&gt;|&lt;/span&gt;
+—————————–+—————————-+————————+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This lists the nodes in our cluster. Currently we have a single node cluster and that node has the control role. fleetctl talks to the host running the fleet service through ssh and gets information about the cluster, so the host running the fleet service must be accessible remotely via ssh.&lt;/p&gt;
&lt;p&gt;The FLEETCTL_TUNNEL parameter specifies that the fleet service is running on a remote server with ip 108.182.62.205 (our control node). Use the floating ip address from the last section for this parameter. More information about configuring fleetctl can be found using the fleetctl client.&lt;/p&gt;
&lt;p&gt;fleetctl can be used to monitor and to start/stop different services on our cluster. Node the ID above and use that to tell fleet to ssh to the control node:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh 0315e138
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lets set some keys and get their values. ETCDCTL_PEERS is a comma separated list of all etcd peers. Currently we have single etcd server running on standard 2379, so we specify https://108.182.62.205:2379 below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nv"&gt;ETCDCTL_PEERS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”https://108.182.62.205:2379″ etcdctl ls
$ etcdctl ls –recursive
$ etcdctl &lt;span class="nb"&gt;set&lt;/span&gt; topic coreos
$ etcdctl get topic
coreos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can use –debug option to understand what API is being called. This gives an overview on etcd’s RESTful api.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl –debug get topic
Cluster-Endpoints: http://108.182.62.205:2379
Curl-Example: curl -X GET http://108.182.62.205:2379/v2/keys/topic?quorum&lt;span class="o"&gt;=&lt;/span&gt;false&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nv"&gt;recursive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;false&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nv"&gt;sorted&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;coreos&lt;/h4&gt;
&lt;p&gt;More information about API can be found at coreos-api.&lt;/p&gt;
&lt;h4&gt;Insight&lt;/h4&gt;
&lt;p&gt;Let’s understand cluster setup and see how CoreOS cluster talks to each other.&lt;/p&gt;
&lt;p&gt;heat template
The heat template is pretty self-explanatory. It consists of all the required services and uses cloud config to initialise data during the CoreOS bootstrap.&lt;/p&gt;
&lt;p&gt;cloud-init
As part of heat-template-control.yaml, we are provisioning a single node with cloud config:&lt;/p&gt;
&lt;h3&gt;cloud-config&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;coreos&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;fleet&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;etcd_servers&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;127.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;control&lt;/span&gt;
&lt;span class="n"&gt;etcd2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;etcd2&lt;/span&gt;
&lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;$token_url&lt;/span&gt;
&lt;span class="n"&gt;advertise&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="n"&gt;$public_ipv4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;advertise&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;peer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="n"&gt;$public_ipv4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2380&lt;/span&gt;
&lt;span class="n"&gt;listen&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;listen&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;peer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2380&lt;/span&gt;
&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="err"&gt;–&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;etcd2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;service&lt;/span&gt;
&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="err"&gt;–&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fleet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;service&lt;/span&gt;
&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
&lt;span class="n"&gt;reboot&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reboot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This follows the standard CoreOS cloud-init guide to initialize a system. Fleet and etcd2 services are already present within CoreOS alpha channel. We try to override the default etcd2 and fleet configuration with custom parameters.&lt;/p&gt;
&lt;p&gt;First two lines are for configuring fleet &amp;amp; etcd2. This fleet running on node about etcd servers (a comma separated servers) and their role. These config are placed in etcd2 service at  /run/systemd/system/fleet.service.d/20-cloud-init and they override during etcd startup. Similarly etcd config are placed at /run/systemd/system/etcd2.service.d/20-cloud-init inside control node for overriding.&lt;/p&gt;
&lt;p&gt;In the unit section, we are passing the command to start both these services.&lt;/p&gt;
&lt;p&gt;CoreOs has an update strategy that consists of three channels; alpha, beta and stable. The alpha channel is the most recent release and it closely tracks current development work. It is released very frequently. The beta channel consists of promoted alpha releases that have received more testing and is released less often that alpha. The stable channel should be used for a stable production CoreOS cluster.&lt;/p&gt;
&lt;p&gt;To see what channel is being used, look into /etc/coreos/update.conf.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This concludes the basic single node setup of a CoreOS cluster. This doesn’t do much but gives us a brief understanding of the underlying concepts of CoreOS. We can verify the status of both the services after sshing into a node and using the following commands: alok@remote $ systemctl status etcd2 &amp;amp;alok@remote $ systemctl status fleet&lt;/p&gt;
&lt;h2&gt;START A MULTI NODE CLUSTER&lt;/h2&gt;
&lt;p&gt;Now that we are confident with CoreOS cloud init, systemd and HEAT templates, lets run a cluster with one control and two worker nodes.&lt;/p&gt;
&lt;p&gt;Delete the old openstack cloud&lt;/p&gt;
&lt;h3&gt;Delete old stack created with single node&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-delete trycoreos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run stack-create for new cluster setup.&lt;/p&gt;
&lt;h3&gt;Create another stack for three nodes&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-create -f heat-template.yaml -P &lt;span class="nv"&gt;discovery_token_url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;curl -q https://discovery.etcd.io/new?size&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt; -P &lt;span class="nv"&gt;public_network_uuid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;87cb4819-33d4-4f2d-86d2-6970c11962da trycoreos
+——————————————————–+———————+——————————+——————————-+
&lt;span class="p"&gt;|&lt;/span&gt; id                                                                  &lt;span class="p"&gt;|&lt;/span&gt; stack_name       &lt;span class="p"&gt;|&lt;/span&gt; stack_status                &lt;span class="p"&gt;|&lt;/span&gt; creation_time                &lt;span class="p"&gt;|&lt;/span&gt;
+——————————————————–+———————+——————————+——————————-+
&lt;span class="p"&gt;|&lt;/span&gt; 897f08ad-4beb-4000-a871-aaa0231ade90 &lt;span class="p"&gt;|&lt;/span&gt; trycoreos          &lt;span class="p"&gt;|&lt;/span&gt; CREATE_IN_PROGRESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2015&lt;/span&gt;-07-30T22:11:35Z &lt;span class="p"&gt;|&lt;/span&gt;
+——————————————————–+———————+——————————+——————————-+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This provisions three node cluster and assuming that control node has ip address 108.182.62.205, lets list machines in cluster&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nv"&gt;FLEETCTL_TUNNEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”108.182.62.205″ fleetctl list-machines
The authenticity of host ‘108.182.62.205’ can’t be established.
RSA key fingerprint is &lt;span class="m"&gt;48&lt;/span&gt;:17:d4:4f:fe:33:0d:b5:44:b3:5b:11:fa:b0:e6:03.
Are you sure you want to &lt;span class="k"&gt;continue&lt;/span&gt; connecting &lt;span class="o"&gt;(&lt;/span&gt;yes/no&lt;span class="o"&gt;)&lt;/span&gt;? yes
Warning: Permanently added ‘108.182.62.205’ &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
MACHINE         IP              METADATA
93c797b0…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.2   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;node
e7d9f87f…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.4   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;control
ee9c8044…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.5   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let’s ssh to one of the nodes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh ee9c8044
The authenticity of host ‘192.168.222.5’ can’t be established.
RSA key fingerprint is &lt;span class="m"&gt;95&lt;/span&gt;:59:c9:ed:ee:ae:4c:5d:b1:db:95:5a:5e:7a:f2:20.
Are you sure you want to &lt;span class="k"&gt;continue&lt;/span&gt; connecting &lt;span class="o"&gt;(&lt;/span&gt;yes/no&lt;span class="o"&gt;)&lt;/span&gt;? yes
Warning: Permanently added ‘192.168.222.5’ &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
CoreOS alpha &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;752&lt;/span&gt;.1.0&lt;span class="o"&gt;)&lt;/span&gt;
Failed Units: &lt;span class="m"&gt;0&lt;/span&gt;
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Sharing/accessing keys across cluster&lt;/h3&gt;
&lt;p&gt;Lets use etcd to list keys, set key and see them from different machines.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl ls –recursive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see there is no keys set as of now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl &lt;span class="nb"&gt;set&lt;/span&gt; coreos/network/config “192.168.3.0/24”
$ etcdctl ls –recursive
/coreos
/coreos/network
/coreos/network/config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let’s verify that these are same inside the control node. To do so we SSH into the worker control node and do a recursive list of keys.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh e7d9f87f
$ etcdctl ls –recursive
/coreos
/coreos/network
/coreos/network/config
$ etcdclt get /coreos/network/config
&lt;span class="m"&gt;192&lt;/span&gt;.168.3.0/24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This brings to an end our very basic CoreOS cluster setup and how to talk to each node in the cluster. We can now use this CoreOS cluster to host applications inside docker containers and to manage them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Credit&lt;/strong&gt;: This is inspired by coreos-heat templates from &lt;a href="https://github.com/sinner-/heat-coreos"&gt;heat-coreos&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="openstack"></category></entry><entry><title>Hello World</title><link href="http://rajalokan.github.io/hello-world.html" rel="alternate"></link><published>2015-08-09T00:00:00+05:30</published><updated>2015-08-09T00:00:00+05:30</updated><author><name>Alok Kumar</name></author><id>tag:rajalokan.github.io,2015-08-09:/hello-world.html</id><content type="html">&lt;p&gt;Hello World!! My First Blog. &lt;/p&gt;
&lt;p&gt;&lt;img alt="helloworld" src="images/helloworld.jpeg"&gt;&lt;/p&gt;</content><category term="helloworld"></category></entry></feed>