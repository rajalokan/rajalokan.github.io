<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>@rajalokan - openstack</title><link href="http://rajalokan.github.io/" rel="alternate"></link><link href="http://rajalokan.github.io/feeds/openstack.atom.xml" rel="self"></link><id>http://rajalokan.github.io/</id><updated>2015-08-13T14:39:00+05:30</updated><subtitle>Alok Kumar</subtitle><entry><title>Getting Started with coreos in OpenStack</title><link href="http://rajalokan.github.io/getting-started-with-coreos-in-openstack.html" rel="alternate"></link><published>2015-08-13T14:39:00+05:30</published><updated>2015-08-13T14:39:00+05:30</updated><author><name>Alok Kumar</name></author><id>tag:rajalokan.github.io,2015-08-13:/getting-started-with-coreos-in-openstack.html</id><summary type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/getting-started-with-coreos-on-openstack/" style="color: #004085; text-decoration: underline;"&gt;getting started with coreos on openstack&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;CoreOS is an open-source lightweight operating system based on the Linux kernel and is designed to provide infrastructure for clustered deployments.&lt;/p&gt;
&lt;p&gt;Microservices architecture have their advantages. In case you are building/managing your stack as containerized …&lt;/p&gt;</summary><content type="html">&lt;div class="alert alert-primary" role="alert" style="color: #004085; background-color: #eff7ff; border-color: #b8daff; position: relative; padding: 0.75rem 1.25rem; margin-bottom: 1.5rem;"&gt;
Reblogged from my blog written for &lt;a href="https://aptira.com/" style="color: #004085; text-decoration: underline;"&gt;@aptira&lt;/a&gt; at &lt;a href="https://aptira.com/getting-started-with-coreos-on-openstack/" style="color: #004085; text-decoration: underline;"&gt;getting started with coreos on openstack&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;CoreOS is an open-source lightweight operating system based on the Linux kernel and is designed to provide infrastructure for clustered deployments.&lt;/p&gt;
&lt;p&gt;Microservices architecture have their advantages. In case you are building/managing your stack as containerized microservices, CoreOS is the perfect operating system. CoreOS provides only the minimal functionality required for deploying applications inside software containers, together with built-in mechanisms for service discovery and configuration sharing.&lt;/p&gt;
&lt;p&gt;Getting started with coreos on openstack (using HEAT)This blog post shares my experiences while learning the basics of CoreOS on an OpenStack infrastructure. On completion of the article, we will have a three node cluster comprised of one control node and two worker nodes. Provisioning of the cluster will be done using OpenStack Heat templates.&lt;/p&gt;
&lt;p&gt;On right is a simple graphics of our end cluster setup.&lt;/p&gt;
&lt;p&gt;The code snippets used in the post are available in my public github account https://github.com/rajalokan/coreos-openstack-beginner for reference.
&lt;img alt="getting_started_with_coreos" src="images/getting_started_with_coreos.png"&gt;&lt;/p&gt;
&lt;h2&gt;PREREQUISITES:&lt;/h2&gt;
&lt;p&gt;To follow this tutorial, we need to install some binaries on the local machine.&lt;/p&gt;
&lt;h3&gt;ETCDCTL&lt;/h3&gt;
&lt;p&gt;etcdctl is a command line client for etcd. CoreOS’s etcd is a distributed, consistent key-value store for shared configuration and service discovery.&lt;/p&gt;
&lt;p&gt;Our control node will have the etcd service running, so on the local machine we need the etcd client installed in order to talk to the CoreOS cluster. We can install this in one step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ curl -L  https://github.com/coreos/etcd/releases/download/v2.1.1/etcd-v2.1.1-linux-amd64.tar.gz -o /tmp/etcd-v2.1.1-linux-amd64.tar.gz
$ tar xzvf /tmp/etcd-v2.1.1-linux-amd64.tar.gz -C /tmp/
$ mv /tmp/etcd-v2.1.1-linux-adm64/etcdctl /usr/local/bin &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x /usr/local/bin/etcdctl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Make sure to add /usr/local/bin to the system PATH or call it directly using &lt;code&gt;/usr/local/bin/etcdctl&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;fleetctl&lt;/h3&gt;
&lt;p&gt;fleet ties together systemd and etcd into a simple distributed init system. Think of it as an extension of systemd that operates at the cluster level instead of at the machine level.&lt;/p&gt;
&lt;p&gt;fleet provides a command-line tool called &lt;code&gt;fleetctl&lt;/code&gt;. We will use this to communicate with our cluster. To install, run the following commands.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ curl -L https://github.com/coreos/fleet/releases/download/v0.11.2/fleet-v0.11.2-linux-amd64.tar.gz -o fleet-v0.11.2-linux-amd64
$ tar xzvf /tmp/fleet-v0.11.2-linux-amd64 -C /tmp/
$ mv /tmp/fleet-v0.11.2-linux-amd64/fleetctl /usr/local/bin &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x /usr/local/bin/fleetctl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;python-heatclient &amp;amp; python-glanceclient&lt;/h3&gt;
&lt;p&gt;We will use the OpenStack heat client to spin up a VM and the OpenStack glance client to create a CoreOS image in our OpenStack infrastructure. We can install both of these client tools using pip:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ pip install python-heatclient python-glanceclient
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we want to add the stable CoreOS image to glance&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ wget http://stable.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2
$ bunzip2 coreos_production_openstack_image.img.bz2
$ glance image-create –name CoreOS –container-format bare  –disk-format qcow2  –file coreos_production_openstack_image.img
$ glance image-list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we should verify that all the binaries are installed correctly. Each of the following commands &lt;code&gt;fleetctl –version&lt;/code&gt; , &lt;code&gt;etcdctl –version&lt;/code&gt;, &lt;code&gt;heat –version&lt;/code&gt; &amp;amp; &lt;code&gt;glance –version&lt;/code&gt; should return output with the version of the binary installed.&lt;/p&gt;
&lt;h2&gt;START A SINGLE NODE CLUSTER&lt;/h2&gt;
&lt;p&gt;Lets keep things simple and start by spinning up a single node cluster (only the control node). This will start a single CoreOS node with fleet and etcd running on it.&lt;/p&gt;
&lt;h3&gt;Start the cluster&lt;/h3&gt;
&lt;p&gt;Starting a cluster through HEAT requires three parameters.&lt;/p&gt;
&lt;h4&gt;Discovery token:&lt;/h4&gt;
&lt;p&gt;For a group of CoreOS machines to form a cluster, their etcd instances need to be connected. We are creating a discovery token for single node to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL.&lt;/p&gt;
&lt;p&gt;In our example, we are using CoreOS’s discovery service to generate token ( curl -q https://discovery.etcd.io/new?size=1). We can as well use our own mechanism to generate token.&lt;/p&gt;
&lt;h4&gt;public network uuid&lt;/h4&gt;
&lt;p&gt;Pass your unique identification of your public network of your OpenStack infrastructure. This is needed to create router inside public network.&lt;/p&gt;
&lt;h4&gt;key_name&lt;/h4&gt;
&lt;p&gt;Also provide your nova keypair’s key_name for ssh access.&lt;/p&gt;
&lt;p&gt;The exact command to start a coreos cluster is shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-create -f heat-template-control.yaml -P &lt;span class="nv"&gt;discovery_token_url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;curl -q https://discovery.etcd.io/new?size&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt; -P &lt;span class="nv"&gt;public_network_uuid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;87cb4819-33d4-4f2d-86d2-6970c11962da trycoreos

+————————————————————+—————–+——————————+————————————-+
&lt;span class="p"&gt;|&lt;/span&gt; id                                                                       &lt;span class="p"&gt;|&lt;/span&gt; stack_name &lt;span class="p"&gt;|&lt;/span&gt; stack_status                 &lt;span class="p"&gt;|&lt;/span&gt;  creation_time                      &lt;span class="p"&gt;|&lt;/span&gt;
+————————————————————+—————–+——————————+————————————-+
&lt;span class="p"&gt;|&lt;/span&gt; 897f08ad-4beb-4000-a871-aaa0231ade90     &lt;span class="p"&gt;|&lt;/span&gt; trycoreos      &lt;span class="p"&gt;|&lt;/span&gt; CREATE_IN_PROGRES   &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2015&lt;/span&gt;-07-30T22:11:35Z        &lt;span class="p"&gt;|&lt;/span&gt;
+————————————————————+—————–+——————————+————————————-+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The initial ‘CREATE_IN_PROGRESS’ stack_status shows that VM provisioning has started. The stack status can be checked with heat stack-show trycoreos. A ‘CREATE_COMPLETE’ stack_status means that our cluster is up. To get the floating ip address of the control node use the command heat output-show trycoreos control_ip.&lt;/p&gt;
&lt;h3&gt;Check the cluster status&lt;/h3&gt;
&lt;p&gt;Now that we have our node ready, we can ssh to it directly by &lt;code&gt;ssh core@&amp;lt;ip_address&amp;gt;&lt;/code&gt;. But let’s try using fleetctl instead to see the status of our cluster and to ssh into the VMs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$  &lt;span class="nv"&gt;FLEETCTL_TUNNEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”108.182.62.205″ fleetctl list-machines

+—————————–+—————————-+————————+
&lt;span class="p"&gt;|&lt;/span&gt;  MACHINE                    &lt;span class="p"&gt;|&lt;/span&gt;        IP                        &lt;span class="p"&gt;|&lt;/span&gt;       METADATA       &lt;span class="p"&gt;|&lt;/span&gt;
+—————————–+—————————-+————————+
&lt;span class="p"&gt;|&lt;/span&gt;  0315e138…                &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;192&lt;/span&gt;.168.222.2           &lt;span class="p"&gt;|&lt;/span&gt;       &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;control   &lt;span class="p"&gt;|&lt;/span&gt;
+—————————–+—————————-+————————+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This lists the nodes in our cluster. Currently we have a single node cluster and that node has the control role. fleetctl talks to the host running the fleet service through ssh and gets information about the cluster, so the host running the fleet service must be accessible remotely via ssh.&lt;/p&gt;
&lt;p&gt;The FLEETCTL_TUNNEL parameter specifies that the fleet service is running on a remote server with ip 108.182.62.205 (our control node). Use the floating ip address from the last section for this parameter. More information about configuring fleetctl can be found using the fleetctl client.&lt;/p&gt;
&lt;p&gt;fleetctl can be used to monitor and to start/stop different services on our cluster. Node the ID above and use that to tell fleet to ssh to the control node:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh 0315e138
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lets set some keys and get their values. ETCDCTL_PEERS is a comma separated list of all etcd peers. Currently we have single etcd server running on standard 2379, so we specify https://108.182.62.205:2379 below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nv"&gt;ETCDCTL_PEERS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”https://108.182.62.205:2379″ etcdctl ls
$ etcdctl ls –recursive
$ etcdctl &lt;span class="nb"&gt;set&lt;/span&gt; topic coreos
$ etcdctl get topic
coreos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can use –debug option to understand what API is being called. This gives an overview on etcd’s RESTful api.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl –debug get topic
Cluster-Endpoints: http://108.182.62.205:2379
Curl-Example: curl -X GET http://108.182.62.205:2379/v2/keys/topic?quorum&lt;span class="o"&gt;=&lt;/span&gt;false&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nv"&gt;recursive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;false&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nv"&gt;sorted&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;coreos&lt;/h4&gt;
&lt;p&gt;More information about API can be found at coreos-api.&lt;/p&gt;
&lt;h4&gt;Insight&lt;/h4&gt;
&lt;p&gt;Let’s understand cluster setup and see how CoreOS cluster talks to each other.&lt;/p&gt;
&lt;p&gt;heat template
The heat template is pretty self-explanatory. It consists of all the required services and uses cloud config to initialise data during the CoreOS bootstrap.&lt;/p&gt;
&lt;p&gt;cloud-init
As part of heat-template-control.yaml, we are provisioning a single node with cloud config:&lt;/p&gt;
&lt;h3&gt;cloud-config&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;coreos&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;fleet&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;etcd_servers&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;127.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;control&lt;/span&gt;
&lt;span class="n"&gt;etcd2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;etcd2&lt;/span&gt;
&lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;$token_url&lt;/span&gt;
&lt;span class="n"&gt;advertise&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="n"&gt;$public_ipv4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;initial&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;advertise&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;peer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="n"&gt;$public_ipv4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2380&lt;/span&gt;
&lt;span class="n"&gt;listen&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2379&lt;/span&gt;
&lt;span class="n"&gt;listen&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;peer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2380&lt;/span&gt;
&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="err"&gt;–&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;etcd2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;service&lt;/span&gt;
&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="err"&gt;–&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fleet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;service&lt;/span&gt;
&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
&lt;span class="n"&gt;reboot&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reboot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This follows the standard CoreOS cloud-init guide to initialize a system. Fleet and etcd2 services are already present within CoreOS alpha channel. We try to override the default etcd2 and fleet configuration with custom parameters.&lt;/p&gt;
&lt;p&gt;First two lines are for configuring fleet &amp;amp; etcd2. This fleet running on node about etcd servers (a comma separated servers) and their role. These config are placed in etcd2 service at  /run/systemd/system/fleet.service.d/20-cloud-init and they override during etcd startup. Similarly etcd config are placed at /run/systemd/system/etcd2.service.d/20-cloud-init inside control node for overriding.&lt;/p&gt;
&lt;p&gt;In the unit section, we are passing the command to start both these services.&lt;/p&gt;
&lt;p&gt;CoreOs has an update strategy that consists of three channels; alpha, beta and stable. The alpha channel is the most recent release and it closely tracks current development work. It is released very frequently. The beta channel consists of promoted alpha releases that have received more testing and is released less often that alpha. The stable channel should be used for a stable production CoreOS cluster.&lt;/p&gt;
&lt;p&gt;To see what channel is being used, look into /etc/coreos/update.conf.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This concludes the basic single node setup of a CoreOS cluster. This doesn’t do much but gives us a brief understanding of the underlying concepts of CoreOS. We can verify the status of both the services after sshing into a node and using the following commands: alok@remote $ systemctl status etcd2 &amp;amp;alok@remote $ systemctl status fleet&lt;/p&gt;
&lt;h2&gt;START A MULTI NODE CLUSTER&lt;/h2&gt;
&lt;p&gt;Now that we are confident with CoreOS cloud init, systemd and HEAT templates, lets run a cluster with one control and two worker nodes.&lt;/p&gt;
&lt;p&gt;Delete the old openstack cloud&lt;/p&gt;
&lt;h3&gt;Delete old stack created with single node&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-delete trycoreos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run stack-create for new cluster setup.&lt;/p&gt;
&lt;h3&gt;Create another stack for three nodes&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heat stack-create -f heat-template.yaml -P &lt;span class="nv"&gt;discovery_token_url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;curl -q https://discovery.etcd.io/new?size&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt; -P &lt;span class="nv"&gt;public_network_uuid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;87cb4819-33d4-4f2d-86d2-6970c11962da trycoreos
+——————————————————–+———————+——————————+——————————-+
&lt;span class="p"&gt;|&lt;/span&gt; id                                                                  &lt;span class="p"&gt;|&lt;/span&gt; stack_name       &lt;span class="p"&gt;|&lt;/span&gt; stack_status                &lt;span class="p"&gt;|&lt;/span&gt; creation_time                &lt;span class="p"&gt;|&lt;/span&gt;
+——————————————————–+———————+——————————+——————————-+
&lt;span class="p"&gt;|&lt;/span&gt; 897f08ad-4beb-4000-a871-aaa0231ade90 &lt;span class="p"&gt;|&lt;/span&gt; trycoreos          &lt;span class="p"&gt;|&lt;/span&gt; CREATE_IN_PROGRESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2015&lt;/span&gt;-07-30T22:11:35Z &lt;span class="p"&gt;|&lt;/span&gt;
+——————————————————–+———————+——————————+——————————-+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This provisions three node cluster and assuming that control node has ip address 108.182.62.205, lets list machines in cluster&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nv"&gt;FLEETCTL_TUNNEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;”108.182.62.205″ fleetctl list-machines
The authenticity of host ‘108.182.62.205’ can’t be established.
RSA key fingerprint is &lt;span class="m"&gt;48&lt;/span&gt;:17:d4:4f:fe:33:0d:b5:44:b3:5b:11:fa:b0:e6:03.
Are you sure you want to &lt;span class="k"&gt;continue&lt;/span&gt; connecting &lt;span class="o"&gt;(&lt;/span&gt;yes/no&lt;span class="o"&gt;)&lt;/span&gt;? yes
Warning: Permanently added ‘108.182.62.205’ &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
MACHINE         IP              METADATA
93c797b0…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.2   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;node
e7d9f87f…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.4   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;control
ee9c8044…     &lt;span class="m"&gt;192&lt;/span&gt;.168.222.5   &lt;span class="nv"&gt;role&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let’s ssh to one of the nodes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh ee9c8044
The authenticity of host ‘192.168.222.5’ can’t be established.
RSA key fingerprint is &lt;span class="m"&gt;95&lt;/span&gt;:59:c9:ed:ee:ae:4c:5d:b1:db:95:5a:5e:7a:f2:20.
Are you sure you want to &lt;span class="k"&gt;continue&lt;/span&gt; connecting &lt;span class="o"&gt;(&lt;/span&gt;yes/no&lt;span class="o"&gt;)&lt;/span&gt;? yes
Warning: Permanently added ‘192.168.222.5’ &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
CoreOS alpha &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;752&lt;/span&gt;.1.0&lt;span class="o"&gt;)&lt;/span&gt;
Failed Units: &lt;span class="m"&gt;0&lt;/span&gt;
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Sharing/accessing keys across cluster&lt;/h3&gt;
&lt;p&gt;Lets use etcd to list keys, set key and see them from different machines.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl ls –recursive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We see there is no keys set as of now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ etcdctl &lt;span class="nb"&gt;set&lt;/span&gt; coreos/network/config “192.168.3.0/24”
$ etcdctl ls –recursive
/coreos
/coreos/network
/coreos/network/config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let’s verify that these are same inside the control node. To do so we SSH into the worker control node and do a recursive list of keys.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ fleetctl ssh e7d9f87f
$ etcdctl ls –recursive
/coreos
/coreos/network
/coreos/network/config
$ etcdclt get /coreos/network/config
&lt;span class="m"&gt;192&lt;/span&gt;.168.3.0/24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This brings to an end our very basic CoreOS cluster setup and how to talk to each node in the cluster. We can now use this CoreOS cluster to host applications inside docker containers and to manage them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Credit&lt;/strong&gt;: This is inspired by coreos-heat templates from &lt;a href="https://github.com/sinner-/heat-coreos"&gt;heat-coreos&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="openstack"></category></entry></feed>